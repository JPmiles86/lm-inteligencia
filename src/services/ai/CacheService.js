// Cache Service - Intelligent caching and performance optimization
// Handles multi-level caching, prompt caching, and performance optimization

import crypto from 'crypto';
import { aiRepository } from '../../repositories/aiRepository.js';

export class CacheService {
  constructor() {
    // Multi-level cache architecture
    this.l1Cache = new Map(); // In-memory, fastest
    this.l2Cache = new Map(); // In-memory, larger capacity
    this.l3Cache = null;      // Database/Redis, persistent
    
    // Cache configuration
    this.config = {
      l1: { maxSize: 100, maxAge: 5 * 60 * 1000 },      // 5 minutes, 100 items
      l2: { maxSize: 1000, maxAge: 30 * 60 * 1000 },    // 30 minutes, 1000 items
      l3: { maxSize: 10000, maxAge: 24 * 60 * 60 * 1000 } // 24 hours, 10000 items
    };
    
    // Cache statistics
    this.stats = {
      hits: { l1: 0, l2: 0, l3: 0 },
      misses: 0,
      evictions: { l1: 0, l2: 0, l3: 0 },
      totalRequests: 0
    };
    
    // Background cleanup
    this.startCleanupTimer();\n    this.initializeL3Cache();\n  }\n\n  async initializeL3Cache() {\n    try {\n      this.l3Cache = await aiRepository.initializeCache();\n    } catch (error) {\n      console.warn('L3 cache initialization failed, continuing without persistent cache:', error.message);\n    }\n  }\n\n  // ================================\n  // CACHE OPERATIONS\n  // ================================\n\n  async get(key, options = {}) {\n    this.stats.totalRequests++;\n    const cacheKey = this.generateCacheKey(key);\n    const { useL1 = true, useL2 = true, useL3 = true } = options;\n    \n    // Try L1 cache first (fastest)\n    if (useL1) {\n      const l1Result = this.l1Cache.get(cacheKey);\n      if (l1Result && !this.isExpired(l1Result)) {\n        this.stats.hits.l1++;\n        l1Result.lastAccessed = Date.now();\n        return l1Result.data;\n      }\n    }\n    \n    // Try L2 cache\n    if (useL2) {\n      const l2Result = this.l2Cache.get(cacheKey);\n      if (l2Result && !this.isExpired(l2Result)) {\n        this.stats.hits.l2++;\n        l2Result.lastAccessed = Date.now();\n        \n        // Promote to L1 cache\n        if (useL1) {\n          this.setL1(cacheKey, l2Result.data);\n        }\n        \n        return l2Result.data;\n      }\n    }\n    \n    // Try L3 cache (database/Redis)\n    if (useL3 && this.l3Cache) {\n      try {\n        const l3Result = await aiRepository.getCacheEntry(cacheKey);\n        if (l3Result && !this.isExpired(l3Result)) {\n          this.stats.hits.l3++;\n          \n          // Promote to L2 and L1\n          if (useL2) {\n            this.setL2(cacheKey, l3Result.data);\n          }\n          if (useL1) {\n            this.setL1(cacheKey, l3Result.data);\n          }\n          \n          return l3Result.data;\n        }\n      } catch (error) {\n        console.warn('L3 cache read failed:', error.message);\n      }\n    }\n    \n    this.stats.misses++;\n    return null;\n  }\n\n  async set(key, data, options = {}) {\n    const cacheKey = this.generateCacheKey(key);\n    const {\n      ttl,\n      priority = 'normal',\n      tags = [],\n      skipL1 = false,\n      skipL2 = false,\n      skipL3 = false\n    } = options;\n    \n    const cacheEntry = {\n      data,\n      createdAt: Date.now(),\n      lastAccessed: Date.now(),\n      ttl: ttl || this.getDefaultTTL(key),\n      priority,\n      tags,\n      size: this.estimateSize(data)\n    };\n    \n    // Set in appropriate cache levels\n    if (!skipL1) {\n      this.setL1(cacheKey, data, cacheEntry);\n    }\n    \n    if (!skipL2) {\n      this.setL2(cacheKey, data, cacheEntry);\n    }\n    \n    if (!skipL3 && this.l3Cache) {\n      try {\n        await aiRepository.setCacheEntry(cacheKey, cacheEntry);\n      } catch (error) {\n        console.warn('L3 cache write failed:', error.message);\n      }\n    }\n  }\n\n  async delete(key) {\n    const cacheKey = this.generateCacheKey(key);\n    \n    // Delete from all cache levels\n    this.l1Cache.delete(cacheKey);\n    this.l2Cache.delete(cacheKey);\n    \n    if (this.l3Cache) {\n      try {\n        await aiRepository.deleteCacheEntry(cacheKey);\n      } catch (error) {\n        console.warn('L3 cache delete failed:', error.message);\n      }\n    }\n  }\n\n  async invalidateByTags(tags) {\n    if (!Array.isArray(tags) || tags.length === 0) return;\n    \n    // Invalidate L1 and L2 caches\n    for (const [key, entry] of this.l1Cache.entries()) {\n      if (entry.tags && entry.tags.some(tag => tags.includes(tag))) {\n        this.l1Cache.delete(key);\n      }\n    }\n    \n    for (const [key, entry] of this.l2Cache.entries()) {\n      if (entry.tags && entry.tags.some(tag => tags.includes(tag))) {\n        this.l2Cache.delete(key);\n      }\n    }\n    \n    // Invalidate L3 cache\n    if (this.l3Cache) {\n      try {\n        await aiRepository.invalidateCacheByTags(tags);\n      } catch (error) {\n        console.warn('L3 cache tag invalidation failed:', error.message);\n      }\n    }\n  }\n\n  // ================================\n  // PROMPT CACHING\n  // ================================\n\n  async cachePromptResponse(prompt, response, context = {}) {\n    const cacheKey = this.generatePromptCacheKey(prompt, context);\n    \n    await this.set(cacheKey, {\n      response,\n      prompt,\n      context,\n      timestamp: Date.now()\n    }, {\n      ttl: context.cacheTTL || 30 * 60 * 1000, // 30 minutes default\n      tags: ['prompt', `provider:${context.provider}`, `task:${context.task}`],\n      priority: 'high'\n    });\n  }\n\n  async getCachedPromptResponse(prompt, context = {}) {\n    const cacheKey = this.generatePromptCacheKey(prompt, context);\n    const cached = await this.get(cacheKey);\n    \n    if (cached) {\n      // Update cache metadata\n      await this.updateCacheMetadata(cacheKey, {\n        lastUsed: Date.now(),\n        useCount: (cached.useCount || 0) + 1\n      });\n    }\n    \n    return cached;\n  }\n\n  generatePromptCacheKey(prompt, context) {\n    // Create a deterministic cache key for prompt + context\n    const keyData = {\n      prompt: prompt.trim(),\n      provider: context.provider,\n      model: context.model,\n      task: context.task,\n      vertical: context.vertical,\n      // Include relevant options that affect output\n      temperature: context.temperature,\n      maxTokens: context.maxTokens,\n      systemInstruction: context.systemInstruction\n    };\n    \n    const keyString = JSON.stringify(keyData, Object.keys(keyData).sort());\n    return 'prompt:' + crypto.createHash('md5').update(keyString).digest('hex');\n  }\n\n  // ================================\n  // CONTEXT CACHING\n  // ================================\n\n  async cacheContext(contextId, contextData) {\n    await this.set(`context:${contextId}`, contextData, {\n      ttl: 60 * 60 * 1000, // 1 hour\n      tags: ['context', `vertical:${contextData.vertical}`],\n      priority: 'high'\n    });\n  }\n\n  async getCachedContext(contextId) {\n    return await this.get(`context:${contextId}`);\n  }\n\n  // ================================\n  // SMART CACHING STRATEGIES\n  // ================================\n\n  async shouldCache(key, data, context = {}) {\n    // Decide whether to cache based on various factors\n    const factors = {\n      size: this.estimateSize(data),\n      complexity: this.estimateComplexity(context),\n      cost: context.cost || 0,\n      frequency: await this.getAccessFrequency(key),\n      provider: context.provider,\n      task: context.task\n    };\n    \n    // Don't cache very large responses (> 100KB)\n    if (factors.size > 100 * 1024) {\n      return false;\n    }\n    \n    // Always cache expensive operations (> $0.01)\n    if (factors.cost > 0.01) {\n      return true;\n    }\n    \n    // Cache frequently accessed items\n    if (factors.frequency > 2) {\n      return true;\n    }\n    \n    // Cache complex operations\n    if (factors.complexity > 0.7) {\n      return true;\n    }\n    \n    // Provider-specific caching rules\n    const providerRules = this.getProviderCachingRules(factors.provider);\n    if (providerRules.alwaysCache?.includes(factors.task)) {\n      return true;\n    }\n    \n    if (providerRules.neverCache?.includes(factors.task)) {\n      return false;\n    }\n    \n    // Default: cache medium complexity and above\n    return factors.complexity > 0.5;\n  }\n\n  estimateComplexity(context) {\n    let score = 0;\n    \n    // Task complexity\n    const taskComplexity = {\n      'title_generation': 0.2,\n      'synopsis_generation': 0.4,\n      'idea_generation': 0.5,\n      'blog_writing_complete': 0.8,\n      'topic_research': 0.9,\n      'image_prompt_generation': 0.6\n    };\n    \n    score += taskComplexity[context.task] || 0.5;\n    \n    // Context size\n    if (context.contextSize) {\n      score += Math.min(context.contextSize / 10000, 0.3);\n    }\n    \n    // Provider-specific complexity\n    if (context.provider === 'perplexity' || context.enableWebSearch) {\n      score += 0.2; // Research tasks are more complex\n    }\n    \n    if (context.enableReasoning || context.thinking) {\n      score += 0.2; // Reasoning adds complexity\n    }\n    \n    return Math.min(score, 1.0);\n  }\n\n  getProviderCachingRules(provider) {\n    const rules = {\n      openai: {\n        alwaysCache: ['blog_writing_complete'],\n        neverCache: ['title_generation'],\n        preferredTTL: 30 * 60 * 1000 // 30 minutes\n      },\n      anthropic: {\n        alwaysCache: ['blog_writing_complete', 'idea_generation'],\n        neverCache: [],\n        preferredTTL: 60 * 60 * 1000 // 1 hour\n      },\n      google: {\n        alwaysCache: ['blog_writing_complete'],\n        neverCache: ['title_generation'],\n        preferredTTL: 45 * 60 * 1000 // 45 minutes\n      },\n      perplexity: {\n        alwaysCache: ['topic_research', 'trend_analysis'],\n        neverCache: [],\n        preferredTTL: 15 * 60 * 1000 // 15 minutes (research data changes quickly)\n      }\n    };\n    \n    return rules[provider] || { alwaysCache: [], neverCache: [], preferredTTL: 30 * 60 * 1000 };\n  }\n\n  // ================================\n  // CACHE MANAGEMENT\n  // ================================\n\n  setL1(key, data, metadata = {}) {\n    if (this.l1Cache.size >= this.config.l1.maxSize) {\n      this.evictL1();\n    }\n    \n    this.l1Cache.set(key, {\n      data,\n      createdAt: Date.now(),\n      lastAccessed: Date.now(),\n      ttl: metadata.ttl || this.config.l1.maxAge,\n      ...metadata\n    });\n  }\n\n  setL2(key, data, metadata = {}) {\n    if (this.l2Cache.size >= this.config.l2.maxSize) {\n      this.evictL2();\n    }\n    \n    this.l2Cache.set(key, {\n      data,\n      createdAt: Date.now(),\n      lastAccessed: Date.now(),\n      ttl: metadata.ttl || this.config.l2.maxAge,\n      ...metadata\n    });\n  }\n\n  evictL1() {\n    // LRU eviction with priority consideration\n    const entries = Array.from(this.l1Cache.entries());\n    \n    // Sort by priority (high priority last) and access time (oldest first)\n    entries.sort((a, b) => {\n      const priorityA = a[1].priority === 'high' ? 1 : 0;\n      const priorityB = b[1].priority === 'high' ? 1 : 0;\n      \n      if (priorityA !== priorityB) {\n        return priorityA - priorityB;\n      }\n      \n      return a[1].lastAccessed - b[1].lastAccessed;\n    });\n    \n    // Remove oldest 10% of entries\n    const removeCount = Math.ceil(this.config.l1.maxSize * 0.1);\n    for (let i = 0; i < removeCount && i < entries.length; i++) {\n      this.l1Cache.delete(entries[i][0]);\n      this.stats.evictions.l1++;\n    }\n  }\n\n  evictL2() {\n    // Similar LRU eviction for L2\n    const entries = Array.from(this.l2Cache.entries());\n    \n    entries.sort((a, b) => {\n      const priorityA = a[1].priority === 'high' ? 1 : 0;\n      const priorityB = b[1].priority === 'high' ? 1 : 0;\n      \n      if (priorityA !== priorityB) {\n        return priorityA - priorityB;\n      }\n      \n      return a[1].lastAccessed - b[1].lastAccessed;\n    });\n    \n    const removeCount = Math.ceil(this.config.l2.maxSize * 0.1);\n    for (let i = 0; i < removeCount && i < entries.length; i++) {\n      this.l2Cache.delete(entries[i][0]);\n      this.stats.evictions.l2++;\n    }\n  }\n\n  startCleanupTimer() {\n    // Clean up expired entries every 5 minutes\n    setInterval(() => {\n      this.cleanupExpiredEntries();\n    }, 5 * 60 * 1000);\n  }\n\n  cleanupExpiredEntries() {\n    const now = Date.now();\n    \n    // Clean L1 cache\n    for (const [key, entry] of this.l1Cache.entries()) {\n      if (this.isExpired(entry, now)) {\n        this.l1Cache.delete(key);\n        this.stats.evictions.l1++;\n      }\n    }\n    \n    // Clean L2 cache\n    for (const [key, entry] of this.l2Cache.entries()) {\n      if (this.isExpired(entry, now)) {\n        this.l2Cache.delete(key);\n        this.stats.evictions.l2++;\n      }\n    }\n  }\n\n  isExpired(entry, now = Date.now()) {\n    return (now - entry.createdAt) > entry.ttl;\n  }\n\n  // ================================\n  // PERFORMANCE OPTIMIZATION\n  // ================================\n\n  async preloadCache(keys) {\n    // Preload frequently accessed items\n    const results = await Promise.allSettled(\n      keys.map(key => this.get(key, { useL1: false, useL2: false, useL3: true }))\n    );\n    \n    results.forEach((result, index) => {\n      if (result.status === 'fulfilled' && result.value) {\n        const key = keys[index];\n        this.setL2(this.generateCacheKey(key), result.value);\n        this.setL1(this.generateCacheKey(key), result.value);\n      }\n    });\n  }\n\n  async warmupCache(patterns = []) {\n    // Warm up cache with common patterns\n    const commonPatterns = [\n      'context:*',\n      'prompt:*',\n      'style_guide:*',\n      ...patterns\n    ];\n    \n    for (const pattern of commonPatterns) {\n      try {\n        const keys = await aiRepository.getCacheKeysByPattern(pattern);\n        await this.preloadCache(keys.slice(0, 50)); // Limit to 50 keys per pattern\n      } catch (error) {\n        console.warn(`Cache warmup failed for pattern ${pattern}:`, error.message);\n      }\n    }\n  }\n\n  // ================================\n  // CACHE STATISTICS & MONITORING\n  // ================================\n\n  getStatistics() {\n    const totalHits = this.stats.hits.l1 + this.stats.hits.l2 + this.stats.hits.l3;\n    const hitRate = this.stats.totalRequests > 0 \n      ? (totalHits / this.stats.totalRequests * 100).toFixed(2) + '%'\n      : '0%';\n    \n    return {\n      hitRate,\n      totalRequests: this.stats.totalRequests,\n      hits: {\n        total: totalHits,\n        l1: this.stats.hits.l1,\n        l2: this.stats.hits.l2,\n        l3: this.stats.hits.l3\n      },\n      misses: this.stats.misses,\n      evictions: this.stats.evictions,\n      cacheSize: {\n        l1: this.l1Cache.size,\n        l2: this.l2Cache.size,\n        l3: 'N/A' // Would need to query database\n      },\n      memoryUsage: this.estimateMemoryUsage()\n    };\n  }\n\n  estimateMemoryUsage() {\n    let totalSize = 0;\n    \n    for (const entry of this.l1Cache.values()) {\n      totalSize += this.estimateSize(entry);\n    }\n    \n    for (const entry of this.l2Cache.values()) {\n      totalSize += this.estimateSize(entry);\n    }\n    \n    return {\n      bytes: totalSize,\n      human: this.formatBytes(totalSize)\n    };\n  }\n\n  resetStatistics() {\n    this.stats = {\n      hits: { l1: 0, l2: 0, l3: 0 },\n      misses: 0,\n      evictions: { l1: 0, l2: 0, l3: 0 },\n      totalRequests: 0\n    };\n  }\n\n  // ================================\n  // UTILITY METHODS\n  // ================================\n\n  generateCacheKey(key) {\n    if (typeof key === 'string') {\n      return key;\n    }\n    \n    return crypto.createHash('md5').update(JSON.stringify(key)).digest('hex');\n  }\n\n  estimateSize(obj) {\n    return JSON.stringify(obj).length * 2; // Rough estimate\n  }\n\n  formatBytes(bytes) {\n    const sizes = ['Bytes', 'KB', 'MB', 'GB'];\n    if (bytes === 0) return '0 Bytes';\n    \n    const i = Math.floor(Math.log(bytes) / Math.log(1024));\n    return Math.round(bytes / Math.pow(1024, i) * 100) / 100 + ' ' + sizes[i];\n  }\n\n  getDefaultTTL(key) {\n    if (typeof key === 'string') {\n      if (key.startsWith('prompt:')) return 30 * 60 * 1000; // 30 minutes\n      if (key.startsWith('context:')) return 60 * 60 * 1000; // 1 hour\n      if (key.startsWith('style_guide:')) return 24 * 60 * 60 * 1000; // 24 hours\n    }\n    \n    return this.config.l2.maxAge; // Default to L2 TTL\n  }\n\n  async getAccessFrequency(key) {\n    try {\n      return await aiRepository.getCacheAccessFrequency(this.generateCacheKey(key));\n    } catch (error) {\n      return 0;\n    }\n  }\n\n  async updateCacheMetadata(key, metadata) {\n    if (this.l3Cache) {\n      try {\n        await aiRepository.updateCacheMetadata(key, metadata);\n      } catch (error) {\n        console.warn('Failed to update cache metadata:', error.message);\n      }\n    }\n  }\n\n  async clear(level = 'all') {\n    switch (level) {\n      case 'l1':\n        this.l1Cache.clear();\n        break;\n      case 'l2':\n        this.l2Cache.clear();\n        break;\n      case 'l3':\n        if (this.l3Cache) {\n          await aiRepository.clearCache();\n        }\n        break;\n      case 'all':\n        this.l1Cache.clear();\n        this.l2Cache.clear();\n        if (this.l3Cache) {\n          await aiRepository.clearCache();\n        }\n        break;\n    }\n  }\n}